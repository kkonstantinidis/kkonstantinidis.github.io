<feed xmlns="http://www.w3.org/2005/Atom"> <id>https://kkonstantinidis.github.io//</id><title>Konstantinos Konstantinidis</title><subtitle>Distributed systems and machine learning enthusiast. Ph.D., Iowa State University.</subtitle> <updated>2024-11-21T08:55:55+00:00</updated> <author> <name>Konstantinos Konstantinidis</name> <uri>https://kkonstantinidis.github.io//</uri> </author><link rel="self" type="application/atom+xml" href="https://kkonstantinidis.github.io//feed.xml"/><link rel="alternate" type="text/html" hreflang="en-US" href="https://kkonstantinidis.github.io//"/> <generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator> <rights> Â© 2024 Konstantinos Konstantinidis </rights> <icon>/assets/img/favicons/favicon.ico</icon> <logo>/assets/img/favicons/favicon-96x96.png</logo> <entry><title>Straggler Mitigation in Matrix Multiplication</title><link href="https://kkonstantinidis.github.io//posts/straggler-mitigation-in-matrix-multiplication/" rel="alternate" type="text/html" title="Straggler Mitigation in Matrix Multiplication" /><published>2021-11-28T18:00:00+00:00</published> <updated>2022-08-18T01:56:15+00:00</updated> <id>https://kkonstantinidis.github.io//posts/straggler-mitigation-in-matrix-multiplication/</id> <content src="https://kkonstantinidis.github.io//posts/straggler-mitigation-in-matrix-multiplication/" /> <author> <name>Konstantinos Konstantinidis</name> </author> <category term="Research" /> <summary> Relevant paper: [J1]. It is well recognized that stragglers, i.e., machines that perform considerably slower than average on a cluster, are a major bottleneck of distributed algorithms. In these cases, the result depends on local computations carried out by all servers; hence, the execution may be delayed if one of them is experiencing performance issues. In order to motivate this problem, we ... </summary> </entry> <entry><title>Robust Machine Learning - Filtering</title><link href="https://kkonstantinidis.github.io//posts/robust-machine-learning-filtering/" rel="alternate" type="text/html" title="Robust Machine Learning - Filtering" /><published>2021-11-28T18:00:00+00:00</published> <updated>2022-08-18T04:21:37+00:00</updated> <id>https://kkonstantinidis.github.io//posts/robust-machine-learning-filtering/</id> <content src="https://kkonstantinidis.github.io//posts/robust-machine-learning-filtering/" /> <author> <name>Konstantinos Konstantinidis</name> </author> <category term="Research" /> <summary> Relevant paper: [C3]. In this work, we examine distributed deep learning setups in which computing devices may return adversarial or erroneous computations. This behavior is called Byzantine and is typically attributed to adversarial attacks on the nodes or failures/crashes thereof. As a result, the corresponding gradients are potentially unreliable for use in training and model updates. Achie... </summary> </entry> <entry><title>Robust Machine Learning - Detection</title><link href="https://kkonstantinidis.github.io//posts/robust-machine-learning-detection/" rel="alternate" type="text/html" title="Robust Machine Learning - Detection" /><published>2021-11-28T18:00:00+00:00</published> <updated>2023-11-25T03:48:37+00:00</updated> <id>https://kkonstantinidis.github.io//posts/robust-machine-learning-detection/</id> <content src="https://kkonstantinidis.github.io//posts/robust-machine-learning-detection/" /> <author> <name>Konstantinos Konstantinidis</name> </author> <category term="Research" /> <summary> Relevant papers: [J3, C4]. The following article examines the problem of Byzantine behavior in distributed learning setups from the lens of detection. The problem has been introduced in another article discussing our previously proposed method ByzShield. In this work, our objective is to initially attempt to detect which nodes behave adversarially or erroneously before resorting to gradient ag... </summary> </entry> <entry><title>Coding &amp; Distributed Computing</title><link href="https://kkonstantinidis.github.io//posts/coding-&amp;-distributed-computing/" rel="alternate" type="text/html" title="Coding &amp; Distributed Computing" /><published>2021-11-28T18:00:00+00:00</published> <updated>2022-08-18T01:56:15+00:00</updated> <id>https://kkonstantinidis.github.io//posts/coding-&amp;-distributed-computing/</id> <content src="https://kkonstantinidis.github.io//posts/coding-&amp;-distributed-computing/" /> <author> <name>Konstantinos Konstantinidis</name> </author> <category term="Research" /> <summary> Relevant papers: [J2], [C1]. Big data analytics often require distributed implementations in which clusters process a sheer amount of data. Hadoop MapReduce and Apache Spark are examples of such frameworks. These systems perform initial processing during the Map phase, the machines communicate during the Shuffle phase, and a final computation is performed in the Reduce phase. Extensive evidenc... </summary> </entry> <entry><title>Aggregated MapReduce</title><link href="https://kkonstantinidis.github.io//posts/aggregated-mapreduce/" rel="alternate" type="text/html" title="Aggregated MapReduce" /><published>2021-11-28T18:00:00+00:00</published> <updated>2022-08-18T01:56:15+00:00</updated> <id>https://kkonstantinidis.github.io//posts/aggregated-mapreduce/</id> <content src="https://kkonstantinidis.github.io//posts/aggregated-mapreduce/" /> <author> <name>Konstantinos Konstantinidis</name> </author> <category term="Research" /> <summary> Relevant papers: [J2], [C2]. We have extended our coded scheme for a general MapReduce framework to a class of distributed algorithms, broadly used in deep learning, where intermediate computations of the same task can be combined. In this case, our goal is to execute multiple MapReduce jobs. Prior techniques reduce the communication load significantly. However, they suffer from a requirement ... </summary> </entry> </feed>

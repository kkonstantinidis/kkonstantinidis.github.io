---
title: Industry
icon: fas fa-briefcase
order: 2
---

## Research Scientist at Meta (Facebook)
Menlo Park, CA | 12/2023 – Present  

I work on the Marketplace Recommendation & Delivery Infrastructure team building highly-scalable data pipelines to compute, store, and retrieve machine learning features & training data, primarily in C++ and Python. Also, I implement system changes, flexible APIs, and monitoring to improve the latency and reliability of offline and online feature serving.

## Software Engineer, Platform at C3 AI
Redwood City, CA | 09/2022 – 11/2023  

As a member of the Platform - Data team, I worked on machine learning infrastructure problems. I researched, built and optimized the _feature store_, a system that stores machine learning features and serves them to models for predicting phenomena or events. My methods transform and fetch the data from the database with low latency. I analyzed the complexity of my algorithms mathematically and experimentally and identified bottlenecks to speed them up and reduce cloud computing costs on AWS and GCP. Finally, I organized meetings with data scientists and engineers to discuss my findings, analyze efficiency and investigate optimizations to reduce computation time across multiple machines.

## Software Engineer Intern (Ph.D.) at Meta (Facebook)
Menlo Park, CA | 05/2022 – 08/2022  

Developed multiple debugging components for machine learning feature authoring used in the data pipelines of Facebook Marketplace. The main component was a framework that categorizes errors during feature compilation, generates alerts, and assigns tasks to the appropriate team; this framework was integrated with the CI/CD. Another end product of my work was an internal UI tool to fetch and display feature values from low-latency storage after a series of transformations.

## Software Engineer Intern, Platform at C3 AI
Redwood City, CA | 06/2021 – 08/2021  

Implemented an end-to-end framework for cluster failure prediction; the framework has two components. The first is the data pipeline which loads cluster health metrics, handles missing data, and creates a training data set. The second component is the ML pipeline which trains a model and makes predictions regarding the cluster's state as soon as new test data becomes available. Followed the process of continuous integration / continuous deployment (CI/CD).

